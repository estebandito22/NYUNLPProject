"""
Interactive script to examine word embeddings generated by Elmo vs our network.
(ATOM EDITOR REQUIRED OR PLACE IN JUPYTER NOTEBOOK).
"""

import os
import json
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import numpy as np
import torch

from nmt.nn.enc_dec import EncDec


cwd = os.getcwd()
elbo_en = np.load(os.path.join(cwd, 'inputs', 'iwslt-vi-en', 'elbo_embds.en.npy'))

# pca = PCA(n_components=2)
# elbo_en_pca = pca.fit_transform(elbo_en)

tsne_elbo = TSNE(n_components=2)
embo_embds_tsne = tsne_elbo.fit_transform(elbo_en)

with open(os.path.join(cwd, 'inputs', 'iwslt-vi-en', 'token2id.en'), 'r') as f:
    id2token = json.load(f)

emo_list = ['happy', 'sad', 'angry', 'joy',
            'desire', 'apathy', 'despair', 'grief', 'cat', 'car', 'truck', 'dog',
            'running', 'flying', 'student', 'teacher', 'driving', 'lion']
emo_ids = [id2token[w] for w in emo_list]
emo_xys = [(embo_embds_tsne[i, 0], embo_embds_tsne[i, 1]) for i in emo_ids]
fig, ax = plt.subplots(figsize=(10,7))
ax.scatter(x = embo_embds_tsne[:, 0], y = embo_embds_tsne[:, 1], alpha=0.1, color='k')
for i, lab in enumerate(emo_list):
    plt.annotate(lab, emo_xys[i], color='r')
plt.xlabel("TSNE-1")
plt.ylabel("TSNE-1")
plt.title("T-SNE of Elmo Word Embeddings")
plt.tight_layout()
plt.savefig('tsne_elmo_word_embeddings.png')






model_dir = os.path.join(cwd, 'outputs_vi/ENCDEC_wed_512_we_False_evs_15319_dvs_17217_ri_False_ehd_512_dhd_1024_enl_2_dnl_2_edo_0.2_ddo_0.2_di_0.1_do_0.1_at_True_bw_1_op_sgd_lr_0.250000001_wd_0.0_cg_0.1_ls_reduce_on_plateau_ml_0.0001_mt_lstm_tf_0.95_ks_0')
epoch = 7

epoch_file = "epoch_"+str(epoch)+".pth"
model_file = os.path.join(model_dir, epoch_file)
with open(model_file, 'rb') as model_dict:
    if torch.cuda.is_available():
        checkpoint = torch.load(model_dict)
    else:
        checkpoint = torch.load(model_dict, map_location='cpu')

checkpoint['state_dict'].keys()

encdec = EncDec()
encdec.load(model_dir, epoch, 1)

decoder_embds = encdec.model.decoder.target_word_embd.embeddings.weight.cpu().detach().numpy()
decoder_embds = encdec.model.encoder.source_word_embd.embeddings.weight.cpu().detach().numpy()
decoder_embds = checkpoint['state_dict']['decoder.hidden2vocab.weight']
decoder_embds.shape

pca = PCA(n_components=2)
decoder_embds_pca = pca.fit_transform(decoder_embds)

tsne = TSNE(n_components=2)
decoder_embds_tsne = tsne.fit_transform(decoder_embds)


emo_list = ['happy', 'sad', 'angry', 'joy',
            'desire', 'apathy', 'despair', 'grief', 'cat', 'car', 'truck', 'dog',
            'running', 'flying', 'student', 'teacher', 'driving', 'lion']
emo_ids = [id2token[w] for w in emo_list]
emo_xys = [(decoder_embds_tsne[i, 0], decoder_embds_tsne[i, 1]) for i in emo_ids]
fig, ax = plt.subplots(figsize=(12,8))
ax.scatter(x=decoder_embds_tsne[:, 0], y=decoder_embds_tsne[:, 1], alpha=0.1, color='k')
for i, lab in enumerate(emo_list):
    plt.annotate(lab, emo_xys[i], color='r')
plt.xlabel("TSNE-1")
plt.ylabel("TSNE-2")
plt.title("T-SNE of Decoder Word Embeddings")
plt.savefig('tsne_decoder_word_embeddings.png')
